{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from feature_engine.selection import DropCorrelatedFeatures\n",
    "import joblib\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV,cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_cols(df):\n",
    "    num_cols = list(df.select_dtypes(include=\"number\"))\n",
    "    cat_cols = [col for col in df.columns if col not in num_cols]\n",
    "    num_but_cat = [col for col in num_cols if df[col].nunique()<10]\n",
    "    cat_but_car = [col for col in cat_cols if df[col].nunique() >20]\n",
    "    cat_cols = cat_cols + num_but_cat \n",
    "    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n",
    "    num_cols = [col for col in num_cols if col not in num_but_cat]\n",
    "    print(f\"cat_cols = {len(cat_cols)}\")\n",
    "    print(f\"num_cols = {len(num_cols)}\")\n",
    "    print(f\"num_but_cat = {len(num_but_cat)}\")\n",
    "    print(f\"cat_but_car= {len(cat_but_car)}\")\n",
    "    return cat_cols,num_cols,cat_but_car,num_but_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cagat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\impute\\_iterative.py:796: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_cols = 4\n",
      "num_cols = 8\n",
      "num_but_cat = 1\n",
      "cat_but_car= 0\n"
     ]
    }
   ],
   "source": [
    "def diabetes_data_prep():\n",
    "    df = pd.read_csv(\"diabetes.csv\")\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "    X = df.drop(\"outcome\",axis=1)\n",
    "    y =df[\"outcome\"]\n",
    "    cols=[\"glucose\",\"bloodpressure\",\"skinthickness\",\"insulin\",\"bmi\"]\n",
    "\n",
    "    for col in cols:\n",
    "        X.loc[X[col]==0,col] = np.nan\n",
    "    def outliers(df,variable):\n",
    "        q1= df[variable].quantile(0.2)\n",
    "        q3 = df[variable].quantile(0.8)\n",
    "        iqr = q3 - q1\n",
    "        lower_lim = q1 - 1.5*iqr\n",
    "        upper_lim = q3 + 1.5*iqr\n",
    "        return lower_lim,upper_lim\n",
    "\n",
    "    def replace_outliers(X,col):\n",
    "        lower_lim,upper_lim = outliers(X,col)\n",
    "        X[col].clip(lower=lower_lim,upper=upper_lim,inplace=True)\n",
    "\n",
    "    replace_outliers(X,\"insulin\")\n",
    "    imp_missforest = IterativeImputer(\n",
    "    estimator=XGBRegressor(n_estimators=300,max_depth=5),\n",
    "    max_iter=30,\n",
    "    initial_strategy=\"median\",\n",
    "    random_state=0\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "    X=imp_missforest.fit_transform(X)\n",
    "    def ohe(dataframe,cat_cols):\n",
    "        dataframe = pd.get_dummies(dataframe,columns=cat_cols,drop_first=True,dtype=int)\n",
    "        return dataframe\n",
    "    X[\"new_glucose_cat\"] = pd.cut(x=X[\"glucose\"],bins=[-1,100,140,200],labels=[\"normal\",\"prediabetes\",\"danger\"])\n",
    "\n",
    "    X.loc[X[\"age\"]<32,\"new_age_cat\"] = 0\n",
    "    X.loc[(X[\"age\"]>=32) & (X[\"age\"]<=50),\"new_age_cat\"]= 1\n",
    "    X.loc[X[\"age\"]>50,\"new_age_cat\"] =2\n",
    "\n",
    "    # X[\"new_age2\"] = pd.cut(x=X[\"age\"],bins=[-1,32,50,100],labels= [0,1,2]) # alt sınıfa dahil eder\n",
    "\n",
    "    X[\"new_bmi\"] = pd.cut(x=X[\"bmi\"],bins=[-1,18.5,24.9,29.9,100],labels=[\"underweight\",\"healthy\",\"overweight\",\"obese\"])\n",
    "    X[\"new_bloodpressure\"] = pd.cut(x=X[\"bloodpressure\"],bins=[-1,79,89,123],labels=[\"normal\",\"hs1\",\"hs2\"])\n",
    "    \n",
    "    cat_cols,num_cols,cat_but_car,num_but_cat = grab_cols(X)\n",
    "    X=ohe(X,cat_cols)\n",
    "    lof = LocalOutlierFactor(n_neighbors=10,n_jobs=-1)\n",
    "    lof.fit_predict(X)\n",
    "    X_scores = lof.negative_outlier_factor_\n",
    "    df = pd.concat([X,y],axis=1)\n",
    "    df=df.drop(labels =list(df[X_scores<-1.8].index),axis=0 )\n",
    "    X=df.drop(\"outcome\",axis=1)\n",
    "    y = df[\"outcome\"]\n",
    "    sc = StandardScaler().set_output(transform=\"pandas\")\n",
    "    X = sc.fit_transform(X)\n",
    "    return X,y\n",
    "X,y = diabetes_data_prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameter optimization\n",
      "##### rf######\n",
      "roc_auc (Before): 0.8298014710157569\n",
      "roc_auc (After): 0.8456871329728474\n",
      "rf best_params: {'max_depth': 5, 'min_samples_split': 15, 'n_estimators': 300}\n",
      "\n",
      "##### xgb######\n",
      "roc_auc (Before): 0.789438862724577\n",
      "roc_auc (After): 0.8463807779522066\n",
      "xgb best_params: {'booster': 'gblinear', 'n_estimators': 200, 'reg_alpha': 0.01, 'reg_lambda': 0.05}\n",
      "\n",
      "##### lr######\n",
      "roc_auc (Before): 0.841388801674516\n",
      "roc_auc (After): 0.8460068608640038\n",
      "lr best_params: {'C': 0.1, 'max_iter': 5000, 'penalty': 'l1'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_params={\"max_depth\":[3,4,5,6], \n",
    "           \"min_samples_split\":[15,20],\n",
    "           \"n_estimators\":[200,300]}\n",
    "\n",
    "xgb_params = {\"booster\":[\"gblinear\",\"gbtree\"],\n",
    "              \"n_estimators\":[200,300],\n",
    "              \"reg_lambda\":[0.02,0.05],\n",
    "              \"reg_alpha\":[0.01,0.02]}\n",
    "\n",
    "lr_params = {'C': [0.01, 0.1, 1, 10],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            \"max_iter\":[5000,1000]}\n",
    "\n",
    "\n",
    "classifiers = [(\"rf\",RandomForestClassifier(class_weight='balanced'),rf_params),\n",
    "               (\"xgb\",XGBClassifier(objective =\"binary:logistic\",scale_pos_weight=1.88),xgb_params),\n",
    "               (\"lr\",LogisticRegression(solver='liblinear',class_weight='balanced'),lr_params)]\n",
    "\n",
    "def hyperparameter_optimization(X,y,scoring=\"roc_auc\"):\n",
    "    print(\"hyperparameter optimization\")\n",
    "    best_models ={}\n",
    "    for name,classifier,params in classifiers:\n",
    "        print(f\"##### {name}######\")\n",
    "        cv_results = cross_val_score(classifier,X,y,scoring=scoring,cv=10,n_jobs=-1).mean()\n",
    "        print(f\"{scoring} (Before): {cv_results}\")\n",
    "        \n",
    "        gs = GridSearchCV(classifier,params,cv=10,scoring=scoring).fit(X,y)\n",
    "        final_model = classifier.set_params(**gs.best_params_)\n",
    "        \n",
    "        cv_results = cross_val_score(final_model,X,y,scoring=scoring,cv=10,n_jobs=-1).mean()\n",
    "        print(f\"{scoring} (After): {cv_results}\")\n",
    "        print(f\"{name} best_params: {gs.best_params_}\", end=\"\\n\\n\")\n",
    "        best_models[name] = final_model\n",
    "    return best_models\n",
    "    \n",
    "best_models = hyperparameter_optimization(X,y,scoring=\"roc_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7776315789473685\n",
      "recall: 0.7767806267806268\n",
      "roc_auc: 0.8494496773068201\n"
     ]
    }
   ],
   "source": [
    "def voting_classifier(best_models,X,y):\n",
    "    voting_clf=VotingClassifier(estimators = [(\"lr\",best_models[\"lr\"]),\n",
    "                                            (\"rf\",best_models[\"rf\"]),\n",
    "                                            (\"xg\",best_models[\"xgb\"])],\n",
    "                              voting='soft',\n",
    "                            weights=[1,1,1])\n",
    "    cv_results = cross_validate(voting_clf,X,y,cv=10,scoring=[\"accuracy\",\"roc_auc\",\"recall\"])\n",
    "    print(f\"accuracy: {cv_results['test_accuracy'].mean()}\")\n",
    "    print(f\"recall: {cv_results['test_recall'].mean()}\")\n",
    "    print(f\"roc_auc: {cv_results['test_roc_auc'].mean()}\")\n",
    "    return voting_clf\n",
    "  \n",
    "voting_clf = voting_classifier(best_models,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    X,y = diabetes_data_prep()\n",
    "    best_models = hyperparameter_optimization(X,y,scoring=\"roc_auc\")\n",
    "    voting_clf = voting_classifier(best_models,X,y)\n",
    "    joblib.dump(voting_clf,\"voting_clf.pkl\")\n",
    "    return voting_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cagat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\impute\\_iterative.py:796: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_cols = 4\n",
      "num_cols = 8\n",
      "num_but_cat = 1\n",
      "cat_but_car= 0\n",
      "hyperparameter optimization\n",
      "##### rf######\n",
      "roc_auc (Before): 0.8468396418396418\n",
      "roc_auc (After): 0.8443767660910517\n",
      "rf best_params: {'max_depth': 4, 'min_samples_split': 15, 'n_estimators': 300}\n",
      "\n",
      "##### xgb######\n",
      "roc_auc (Before): 0.8463807779522066\n",
      "roc_auc (After): 0.8463807779522066\n",
      "xgb best_params: {'booster': 'gblinear', 'n_estimators': 200, 'reg_alpha': 0.01, 'reg_lambda': 0.05}\n",
      "\n",
      "##### lr######\n",
      "roc_auc (Before): 0.8460824466538751\n",
      "roc_auc (After): 0.8460824466538753\n",
      "lr best_params: {'C': 0.1, 'max_iter': 5000, 'penalty': 'l1'}\n",
      "\n",
      "accuracy: 0.7763157894736842\n",
      "recall: 0.7806267806267806\n",
      "roc_auc: 0.8493038548752836\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7763, Recall: 0.8148, ROC AUC: 0.8866, ratio: 1.8861\n",
      "Accuracy: 0.7632, Recall: 0.8148, ROC AUC: 0.8322, ratio: 1.8861\n",
      "Accuracy: 0.8026, Recall: 0.7778, ROC AUC: 0.8813, ratio: 1.8861\n",
      "Accuracy: 0.6974, Recall: 0.7778, ROC AUC: 0.7823, ratio: 1.8861\n",
      "Accuracy: 0.7500, Recall: 0.8462, ROC AUC: 0.8585, ratio: 1.8739\n",
      "Accuracy: 0.8158, Recall: 0.8077, ROC AUC: 0.8515, ratio: 1.8739\n",
      "Accuracy: 0.7632, Recall: 0.7692, ROC AUC: 0.8600, ratio: 1.8739\n",
      "Accuracy: 0.7632, Recall: 0.7692, ROC AUC: 0.8108, ratio: 1.8739\n",
      "Accuracy: 0.7895, Recall: 0.7308, ROC AUC: 0.8200, ratio: 1.8739\n",
      "Accuracy: 0.7763, Recall: 0.6538, ROC AUC: 0.8400, ratio: 1.8739\n",
      "\n",
      "Average Metrics:\n",
      "Accuracy: 0.7697\n",
      "Recall: 0.7762\n",
      "ROC AUC: 0.8423\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score\n",
    "\n",
    "# Stratified K-Fold oluşturma (sınıf dağılımını korur)\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Metrikleri saklamak için boş listeler\n",
    "accuracies = []\n",
    "recalls = []\n",
    "roc_aucs = []\n",
    "\n",
    "for train_idx, test_idx in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # scale_pos_weight hesaplama\n",
    "    ratio = float(y_train.loc[y_train == 0].shape[0]) / y_train.loc[y_train == 1].shape[0]\n",
    "    \n",
    "    clf = xgb.XGBClassifier(scale_pos_weight=ratio,booster=\"gblinear\")\n",
    "    \n",
    "    # Modeli eğit\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Test verisi üzerinde tahmin yap\n",
    "    predictions = clf.predict(X_test)\n",
    "    probas = clf.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Metrikleri hesapla\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "    rec = recall_score(y_test, predictions)\n",
    "    roc_auc = roc_auc_score(y_test, probas)\n",
    "    \n",
    "    accuracies.append(acc)\n",
    "    recalls.append(rec)\n",
    "    roc_aucs.append(roc_auc)\n",
    "\n",
    "    print(f\"Accuracy: {acc:.4f}, Recall: {rec:.4f}, ROC AUC: {roc_auc:.4f}, ratio: {ratio:.4f}\")\n",
    "\n",
    "# Ortalama metrik değerlerini yazdırma\n",
    "print(\"\\nAverage Metrics:\")\n",
    "print(f\"Accuracy: {sum(accuracies)/len(accuracies):.4f}\")\n",
    "print(f\"Recall: {sum(recalls)/len(recalls):.4f}\")\n",
    "print(f\"ROC AUC: {sum(roc_aucs)/len(roc_aucs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 1, 0, 0, 0, 0, 0, 1, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Örnek y verisi\n",
    "y_train = np.array([0, 1, 2, 1, 0,0,0,0,0,1,2,])  # asıl y_train veriniz bu şekilde olacaktır.\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 3, 2], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts = np.bincount(y_train)\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_count = min(class_counts)\n",
    "min_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.66666667, 1.        ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = min_count / class_counts\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.66666667, 1.        , 0.66666667, 0.33333333,\n",
       "       0.33333333, 0.33333333, 0.33333333, 0.33333333, 0.66666667,\n",
       "       1.        ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Örnek y verisi\n",
    "y_train = np.array([0, 1, 2, 1, 0,0,0,0,0,1,2,])  # asıl y_train veriniz bu şekilde olacaktır.\n",
    "\n",
    "# Sınıf frekanslarını hesaplama\n",
    "class_counts = np.bincount(y_train)\n",
    "\n",
    "# En az örneğe sahip sınıfın frekansı\n",
    "min_count = min(class_counts)\n",
    "\n",
    "# Sınıflar için ağırlık faktörleri\n",
    "weights = min_count / class_counts\n",
    "\n",
    "# Eğitim verisine ilgili ağırlıkları atama\n",
    "sample_weights = np.array([weights[i] for i in y_train])\n",
    "sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "clf.fit(X_train, y_train, sample_weight=sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from feature_engine.selection import DropCorrelatedFeatures\n",
    "import joblib\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV,cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "def grab_cols(df):\n",
    "    num_cols = list(df.select_dtypes(include=\"number\"))\n",
    "    cat_cols = [col for col in df.columns if col not in num_cols]\n",
    "    num_but_cat = [col for col in num_cols if df[col].nunique()<10]\n",
    "    cat_but_car = [col for col in cat_cols if df[col].nunique() >20]\n",
    "    cat_cols = cat_cols + num_but_cat \n",
    "    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n",
    "    num_cols = [col for col in num_cols if col not in num_but_cat]\n",
    "    print(f\"cat_cols = {len(cat_cols)}\")\n",
    "    print(f\"num_cols = {len(num_cols)}\")\n",
    "    print(f\"num_but_cat = {len(num_but_cat)}\")\n",
    "    print(f\"cat_but_car= {len(cat_but_car)}\")\n",
    "    return cat_cols,num_cols,cat_but_car,num_but_cat\n",
    "\n",
    "def diabetes_data_prep():\n",
    "    df = pd.read_csv(\"diabetes.csv\")\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "    X = df.drop(\"outcome\",axis=1)\n",
    "    y =df[\"outcome\"]\n",
    "    cols=[\"glucose\",\"bloodpressure\",\"skinthickness\",\"insulin\",\"bmi\"]\n",
    "\n",
    "    for col in cols:\n",
    "        X.loc[X[col]==0,col] = np.nan\n",
    "    def outliers(df,variable):\n",
    "        q1= df[variable].quantile(0.2)\n",
    "        q3 = df[variable].quantile(0.8)\n",
    "        iqr = q3 - q1\n",
    "        lower_lim = q1 - 1.5*iqr\n",
    "        upper_lim = q3 + 1.5*iqr\n",
    "        return lower_lim,upper_lim\n",
    "\n",
    "    def replace_outliers(X,col):\n",
    "        lower_lim,upper_lim = outliers(X,col)\n",
    "        X[col].clip(lower=lower_lim,upper=upper_lim,inplace=True)\n",
    "\n",
    "    replace_outliers(X,\"insulin\")\n",
    "    imp_missforest = IterativeImputer(\n",
    "    estimator=XGBRegressor(n_estimators=300,max_depth=5),\n",
    "    max_iter=30,\n",
    "    initial_strategy=\"median\",\n",
    "    random_state=0\n",
    "    ).set_output(transform=\"pandas\")\n",
    "\n",
    "    X=imp_missforest.fit_transform(X)\n",
    "    def ohe(dataframe,cat_cols):\n",
    "        dataframe = pd.get_dummies(dataframe,columns=cat_cols,drop_first=True,dtype=int)\n",
    "        return dataframe\n",
    "    X[\"new_glucose_cat\"] = pd.cut(x=X[\"glucose\"],bins=[-1,100,140,200],labels=[\"normal\",\"prediabetes\",\"danger\"])\n",
    "\n",
    "    X.loc[X[\"age\"]<32,\"new_age_cat\"] = 0\n",
    "    X.loc[(X[\"age\"]>=32) & (X[\"age\"]<=50),\"new_age_cat\"]= 1\n",
    "    X.loc[X[\"age\"]>50,\"new_age_cat\"] =2\n",
    "\n",
    "    # X[\"new_age2\"] = pd.cut(x=X[\"age\"],bins=[-1,32,50,100],labels= [0,1,2]) # alt sınıfa dahil eder\n",
    "\n",
    "    X[\"new_bmi\"] = pd.cut(x=X[\"bmi\"],bins=[-1,18.5,24.9,29.9,100],labels=[\"underweight\",\"healthy\",\"overweight\",\"obese\"])\n",
    "    X[\"new_bloodpressure\"] = pd.cut(x=X[\"bloodpressure\"],bins=[-1,79,89,123],labels=[\"normal\",\"hs1\",\"hs2\"])\n",
    "    \n",
    "    cat_cols,num_cols,cat_but_car,num_but_cat = grab_cols(X)\n",
    "    X=ohe(X,cat_cols)\n",
    "    lof = LocalOutlierFactor(n_neighbors=10,n_jobs=-1)\n",
    "    lof.fit_predict(X)\n",
    "    X_scores = lof.negative_outlier_factor_\n",
    "    df = pd.concat([X,y],axis=1)\n",
    "    df=df.drop(labels =list(df[X_scores<-1.8].index),axis=0 )\n",
    "    X=df.drop(\"outcome\",axis=1)\n",
    "    y = df[\"outcome\"]\n",
    "    sc = StandardScaler().set_output(transform=\"pandas\")\n",
    "    X = sc.fit_transform(X)\n",
    "    return X,y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def hyperparameter_optimization(X,y,scoring=\"roc_auc\"):\n",
    "    rf_params={\"max_depth\":[3,4,5,6], \n",
    "           \"min_samples_split\":[15,20],\n",
    "           \"n_estimators\":[200,300]}\n",
    "\n",
    "    xgb_params = {\"booster\":[\"gblinear\",\"gbtree\"],\n",
    "              \"n_estimators\":[200,300],\n",
    "              \"reg_lambda\":[0.02,0.05],\n",
    "              \"reg_alpha\":[0.01,0.02]}\n",
    "\n",
    "    lr_params = {'C': [0.01, 0.1, 1, 10],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            \"max_iter\":[5000,1000]}\n",
    "\n",
    "\n",
    "    classifiers = [(\"rf\",RandomForestClassifier(class_weight='balanced'),rf_params),\n",
    "               (\"xgb\",XGBClassifier(objective =\"binary:logistic\",scale_pos_weight=1.88),xgb_params),\n",
    "               (\"lr\",LogisticRegression(solver='liblinear',class_weight='balanced'),lr_params)]\n",
    "    print(\"hyperparameter optimization\")\n",
    "    best_models ={}\n",
    "    for name,classifier,params in classifiers:\n",
    "        print(f\"##### {name}######\")\n",
    "        cv_results = cross_val_score(classifier,X,y,scoring=scoring,cv=10,n_jobs=-1).mean()\n",
    "        print(f\"{scoring} (Before): {cv_results}\")\n",
    "        \n",
    "        gs = GridSearchCV(classifier,params,cv=10,scoring=scoring).fit(X,y)\n",
    "        final_model = classifier.set_params(**gs.best_params_)\n",
    "        \n",
    "        cv_results = cross_val_score(final_model,X,y,scoring=scoring,cv=10,n_jobs=-1).mean()\n",
    "        print(f\"{scoring} (After): {cv_results}\")\n",
    "        print(f\"{name} best_params: {gs.best_params_}\", end=\"\\n\\n\")\n",
    "        best_models[name] = final_model\n",
    "    return best_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
